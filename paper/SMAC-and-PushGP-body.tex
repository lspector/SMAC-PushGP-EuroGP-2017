\section{Introduction}
\label{sec:introduction}

There is a long history of optimizing parameters in evolutionary 
computation~\cite{lobo2007parameter}
including, for example, early work on optimizing mutation 
rates~\cite{back1993optimal} and population sizes for genetic algorithms~\cite{alander1992optimal}.
Recent developments in statistical modeling and machine learning have
led to the design of powerful new techniques for parameter optimization. 
Sequential Model-based Algorithm Configuration (SMAC), for example, is a
highly flexible tool for optimizing algorithm parameters, using repeated runs
of the target algorithm with different parameter values to estimate
the relationship between parameters and performance~\cite{HutHooLey11-SMAC}. In his GECCO 2016 keynote 
address~\cite{Hoos:2016:TCM:2908812.2908960}, Holger Hoos argued
that the parameter optimization field had reached a state where, 
instead of avoiding new
parameters, or making arbitrary choices for parameter values, researchers
should expose as many parameters as possible, and then use tools like
SMAC to optimize those (possibly large) sets of parameters.

Here we summarize a case study of applying SMAC to optimize a
set of 9 parameters for the Clojush\footnote{https://github.com/lspector/Clojush} 
implementation of the PushGP system~\cite{spector:2002:GPEM,push3gecco}
when applied to several software synthesis 
benchmark problems~\cite{Helmuth:2015:GECCO}. While SMAC was able to
find ``tuned'' parameters for one problem which substantially improved
the success rate on that problem, those ``tuned'' parameters appear to
be problem specific, leading to no improvement on several other
problems, and actively hurting performance on another. This is a useful
reminder that parameter optimization is, like all machine learning problems,
subject to overfitting, and that just because a set of parameters works 
well on one problem, or even a set of problems, doesn't mean it will be
a good choice for a new unsolved problem.

\section{Experimental results}
\label{sec:experiments}

\begin{table}[t]
	\caption{Clojush parameters optimized by SMAC for the Replace Space With 
	Newline problem, along with their ranges (as given to SMAC), their 
	default values, and their ``optimized'' values as discovered
	by SMAC.  Population size and
	alignment deviation are specified to have integer values.  Population
	size is also specified to be on a $\log$ scale. The selection
	method is a categorical (non-numeric) variable. The default
	values are what were used in~\cite{Helmuth:2015:GECCO}. 
	The final row shows the number of successes out of 110
	independent trials for each collection of parameter settings.
}
	\begin{center}
		\begin{tabular}{p{3.25cm} p{1.5cm} l l}
			\textbf{Parameter} & \textbf{Range} & \textbf{Default} & \textbf{SMAC} \\
			\midrule
			Population size & $[1, 30K]$ & 1,000 & 150 \\
			% Maximum generations & $[1, 30K]$ & $30,000/\textrm{population size}$ \\
			Selection method & \emph{tournament} \linebreak or \emph{lexicase} & \emph{lexicase} & \emph{lexicase} \\
			\midrule
			Uniform mutation prob. & $[0, 1]$ & 0.2 & 0.36 \\
			Uniform close mut. prob. & $[0, 1]$ & 0.1 & 0.06 \\
			Alternation prob. & $[0, 1]$ & 0.2 & 0.03 \\
			(Alt. + Uni. mut.) prob. & $[0, 1]$ & 0.5 & 0.54 \\
			\midrule
			Alternation rate & $[0, 1]$ & 0.01 & 0.01 \\
			Alignment deviation & $[0, 400]$ & 10 & 121 \\
			\midrule
			Uniform mutation rate & $[0, 1]$ & 0.1 & 0.188 \\
			\midrule
			\noalign{\medskip}
			\textbf{Successes (out of 110)} & & \textbf{59} & \textbf{104} \\
			\bottomrule
		\end{tabular}
	\end{center}
	\label{tab:RswnResults}
\end{table}

We initially used SMAC to optimize nine
PushGP parameters~\cite{Helmuth:2015:dissertation}
on the Replace Space With Newline
software synthesis problem~\cite{Helmuth:2015:GECCO}. Both the default
parameter settings used in previous work~\cite{Helmuth:2015:GECCO} and the 
parameter settings ``discovered'' by SMAC are listed in 
Table~\ref{tab:RswnResults}.

To ensure that changing the population size didn't affect the overall
computational budget for the runs, the number of generations was calculated
so the product of the population size and
number of generations never exceeded the 300,000 individuals processed when using the default parameters
(population size of 1,000 for 300 generations).
The parameters \textit{uniform mutation probability}, \textit{uniform close
mutation probability}, \textit{alternation probability}, and \textit{alternation followed by uniform mutation probability} need to add up
to 1; we let SMAC explore any values in the range $[0, 1]$ and then normalized
those four values so they summed to 1.

%\begin{figure}
%	\includegraphics[width=3in]{../figures/successGenerations}
%	\caption{Cumulative success counts over time for both the default
%		(i.e., ``Standard'')
%		and the SMAC parameter settings.}
%	\label{fig:successGenerations}
%\end{figure}

The final row of Table~\ref{tab:RswnResults} shows that the SMAC parameters
led to significantly higher success rates, with 95\% of
the SMAC parameter runs finding a solution, versus 54\% with the default settings.
The runs using the SMAC
parameter configuration also discovered solutions much earlier than runs using
the default parameters. By generation 100, for example, 87\% of the runs had succeeded when using the SMAC parameters, 
where only 37\% of the runs with the default settings
had succeeded.

\begin{table}[t]
	\centering
	\caption{Number of successful programs out of 100 runs with default parameters, and with the parameters that SMAC found when run on RSWN
		; see Table~\ref{tab:RswnResults}.}
	\label{table:allResults}
	\begin{tabular}{l r r}
		\textbf{Problem} & \textbf{Standard} & \textbf{SMAC} \\
		\textit{Replace Space with Newline} & \textit{54} & \textit{91} \\
		Double Letters	& 0 & 6 \\
		String Lengths Backwards & 68 & 75 \\
		Syllables & 22 & 17 \\
		X-Word Lines & 17 & 3 \\
		\bottomrule
	\end{tabular}
\end{table}

Table~\ref{table:allResults} shows the success rates on 100 independent runs
for each parameter set on four additional software synthesis benchmark
problems~\cite{Helmuth:2015:GECCO}. Here we see that settings
``tuned'' by SMAC for Replace Space With Newline do \emph{not} generalize
well across the new problems. The SMAC settings are significantly
(but not dramatically) better on
Double Letters, and significantly worse on X-Word Lines, while the differences for
the other two problems weren't statistically significant.\footnote{Using a
	pairwise $\chi^2$ test of proportions with a significance level (before
	adjustment) of 0.05.}

\section{Conclusions and Future Work}
\label{sec:conclusions}

SMAC was able to discover parameter settings that substantially improved
performance on a specific problem (Replace Space With Newline) where we already
had a reasonable success rate. Those new parameter settings did not generalize
to additional problems, however, suggesting that SMAC, like most machine
learning systems, is susceptible to ``overfitting''.

SMAC does support a training mode where it can explore parameters across a range
of problems or problem instances, which might help address these ``overfitting''
issues and help SMAC find parameters that perform well more generally.
Unfortunately, running SMAC across a broad
collection of problems requires substantial computational effort. This 
problem is then made worse when several of the test problems have low success rates, as these runs often 
consume considerable computational effort before they fail, 
generating very little new information in the process.
Limited exploratory work in this direction has been held up by SMAC's need for 
an adequate amount of information on successful runs.  For these software 
synthesis problems, this can lead to hundreds of runs, each run lasting up to 
24 hours.

Thus, while it is important to find ways to help SMAC (or similar tools)
discover more generally applicable parameter settings, doing so on suites 
of problems that are computationally expensive to run and have low success rates is certainly problematic, deserving of further attention.

\section*{Acknowledgements}
\label{sec:acknowledgements}

We are extremely grateful to participants on the SMAC forum for their patience
in answering numerous questions as we learned our way around
SMAC. Thanks also to Mitchell Finzel and Elsa Browning for their editing assistance.

This material is based upon work supported by the National Science Foundation under Grants No. 1617087, 1129139 and 1331283. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the National Science Foundation.